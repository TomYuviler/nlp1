# -*- coding: utf-8 -*-
"""NLP1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wr0-zde06PyNcJjGVcId9huiA9CJ-wMK

<a href="https://colab.research.google.com/github/eyalbd2/097215_Natural-Language-Processing_Workshop-Notebooks/blob/master/NLP_1_POS_Tagger.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# <img src="https://img.icons8.com/dusk/64/000000/mind-map.png" style="height:50px;display:inline"> IE 097215 - Technion - Natural Language Processing

## Part 0 - Project Structure
Part Of Speech (POS) tagger is a well known NLP task. As a result, many solutions were proposed to this setup. We present a general solution guidelines to this task (while this is definately not obligatory to use these guidelines to solve HW1). \
A POS tagger can be divided to stages:


*   Pre-training:
    1.   Preprocessing data
    2.   Features engineering  
    3.   Define the objective of the model


*   During training:
    1.   Represent data as feature vectors (Token2Vector) 
    2.   Optimization - We need to tune the weights of the model inorder to solve the objective

*   Inference:
    1.   Use dynamic programing (Viterbi) to tag new data based on MEMM
"""

# Anaconda Environment Setup for HW1 (on Azure machine or on your laptop)

#conda env create --file /datashare/hw1/nlp_hw1_env.yml
#conda activate nlp_hw1_env

"""## Part 1 - Defining and Extracting Features
In class we saw the importance of extracting good features for NLP task. A good feature is such that (1) appear many times in the data and (2) gives information that is relevant for the label.

### Counting feature appearances in data
We would like to include features that appear many times in the data. Hence we first count the number of appearances for each feature. \
This is done at pre-training step.
"""

import numpy as np
import math
import re


def split(x):
  return(re.split(' |\n',x))
def split1(x):
  return (re.split('_',x))

def get_tags_list(file_path):
  tags_list=[]
  with open('/content/test1.wtag') as f:
    for line in f:
      splited_words = split(line)
      del splited_words[-1]
      for word_idx in range(len(splited_words)):
        cur_word, cur_tag = split1(splited_words[word_idx])
        tags_list.append(cur_tag)
    
  return set(tags_list)

from collections import OrderedDict

class feature_statistics_class():

    def __init__(self):
        self.n_total_features = 0  # Total number of features accumulated

        # Init all features dictionaries
        self.words_tags_count_dict = OrderedDict()
        self.spelling_prefix_count_dict = OrderedDict()
        self.spelling_suffix_count_dict = OrderedDict()
        self.trigram_tags_count_dict = OrderedDict()
        self.bigram_tags_count_dict = OrderedDict()
        self.unigram_tags_count_dict = OrderedDict()


        # ---Add more count dictionaries here---
        

    def get_word_tag_pair_count(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
              splited_words = split(line)
              del splited_words[-1]
              for word_idx in range(len(splited_words)):
                  cur_word, cur_tag = split1(splited_words[word_idx])
                  if (cur_word, cur_tag) not in self.words_tags_count_dict:
                      self.words_tags_count_dict[(cur_word, cur_tag)] = 1
                  else:
                      self.words_tags_count_dict[(cur_word, cur_tag)] += 1
      
    def get_spelling_prefix_count(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
              splited_words = split(line)
              del splited_words[-1]
              for word_idx in range(len(splited_words)):
                  cur_word, cur_tag = split1(splited_words[word_idx])
                  length=len(cur_word)
                  for i in range (min(4,length)):
                    if (cur_word[:i+1], cur_tag) not in self.spelling_prefix_count_dict:
                      self.spelling_prefix_count_dict[(cur_word[:i+1], cur_tag)] = 1
                    else:
                      self.spelling_prefix_count_dict[(cur_word[:i+1], cur_tag)] += 1         
                                        
    def get_spelling_suffix_count(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
              splited_words = split(line)
              del splited_words[-1]
              for word_idx in range(len(splited_words)):
                  cur_word, cur_tag = split1(splited_words[word_idx])
                  length=len(cur_word)
                  for i in range (min(4,length)):
                    if (cur_word[-i-1:], cur_tag) not in self.spelling_suffix_count_dict:
                      self.spelling_suffix_count_dict[(cur_word[-i-1:], cur_tag)] = 1
                    else:
                      self.spelling_suffix_count_dict[(cur_word[-i-1:], cur_tag)] += 1  

    def get_trigram_tags_count(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
              splited_words = split(line)
              del splited_words[-1]
              for word_idx in range(len(splited_words)):
                  cur_word, cur_tag = split1(splited_words[word_idx]) 
                  if word_idx>1:
                    ptag =  split1(splited_words[word_idx-1])[1]
                    pptag =  split1(splited_words[word_idx-2])[1]
                  elif word_idx == 1:
                    ptag =  split1(splited_words[word_idx-1])[1]
                    pptag = '*'
                  else:    
                    ptag = '*'
                    pptag = '*'
                  if (pptag,ptag,cur_tag) not in self.trigram_tags_count_dict:
                    self.trigram_tags_count_dict[(pptag,ptag,cur_tag)] = 1
                  else:
                    self.trigram_tags_count_dict[(pptag,ptag,cur_tag)] += 1

    def get_bigram_tags_count(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
              splited_words = split(line)
              del splited_words[-1]
              for word_idx in range(len(splited_words)):
                  cur_word, cur_tag = split1(splited_words[word_idx]) 
                  if word_idx>0:
                    ptag =  split1(splited_words[word_idx-1])[1]
                  else:    
                    ptag = '*'
                  if (ptag,cur_tag) not in self.bigram_tags_count_dict:
                    self.bigram_tags_count_dict[(ptag,cur_tag)] = 1
                  else:
                    self.bigram_tags_count_dict[(ptag,cur_tag)] += 1 

    def get_unigram_tags_count(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
              splited_words = split(line)
              del splited_words[-1]
              for word_idx in range(len(splited_words)):
                  cur_word, cur_tag = split1(splited_words[word_idx]) 
                  if (cur_tag) not in self.unigram_tags_count_dict:
                    self.unigram_tags_count_dict[(cur_tag)] = 1
                  else:
                    self.unigram_tags_count_dict[(cur_tag)] += 1 

    # --- ADD YOURE CODE BELOW --- #

word='yuviler'
print(word[-3-1:])

"""### Indexing features 
After getting feature statistics, each feature is given an index to represent it. We include only features that appear more times in text than the lower bound - 'threshold'
"""

class feature2id_class():

    def __init__(self, feature_statistics, threshold):
        self.feature_statistics = feature_statistics  # statistics class, for each featue gives empirical counts
        self.threshold = threshold                    # feature count threshold - empirical count must be higher than this

        self.n_total_features = 0                     # Total number of features accumulated
        self.n_tag_pairs = 0                          # Number of Word\Tag pairs features
        self.n_prefix_tag = 0
        self.n_suffix_tag = 0
        self.n_trigram_tags = 0
        self.n_bigram_tags = 0
        self.n_unigram_tags = 0

        # Init all features dictionaries
        self.words_tags_dict = OrderedDict()
        self.prefix_tag_dict = OrderedDict()
        self.suffix_tag_dict = OrderedDict()
        self.trigram_tags_dict = OrderedDict()
        self.bigram_tags_dict = OrderedDict()
        self.unigram_tags_dict = OrderedDict()


    def get_word_tag_pairs(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
            splited_words = split(line)
            del splited_words[-1]
            for word_idx in range(len(splited_words)):
                cur_word, cur_tag = split1(splited_words[word_idx])
                if ((cur_word, cur_tag) not in self.words_tags_dict) \
                        and (self.feature_statistics.words_tags_count_dict[(cur_word, cur_tag)] >= self.threshold):
                    self.words_tags_dict[(cur_word, cur_tag)] = self.n_tag_pairs
                    self.n_tag_pairs += 1    
        self.n_total_features += self.n_tag_pairs
        

    def get_prefix_tag_pairs(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
            splited_words = split(line)
            del splited_words[-1]
            for word_idx in range(len(splited_words)):
                cur_word, cur_tag = split1(splited_words[word_idx])
                length = len(cur_word)
                for i in range (min(4,length)):
                  if ((cur_word[:i+1], cur_tag) not in self.prefix_tag_dict) \
                        and (self.feature_statistics.spelling_prefix_count_dict[(cur_word[:i+1], cur_tag)] >= self.threshold):
                    self.prefix_tag_dict[(cur_word[:i+1], cur_tag)] = self.n_total_features + self.n_prefix_tag
                    self.n_prefix_tag += 1
        self.n_total_features = self.n_total_features + self.n_prefix_tag    

    def get_suffix_tag_pairs(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
            splited_words = split(line)
            del splited_words[-1]
            for word_idx in range(len(splited_words)):
                cur_word, cur_tag = split1(splited_words[word_idx])
                length = len(cur_word)
                for i in range (min(4,length)):
                  if ((cur_word[-i-1:], cur_tag) not in self.suffix_tag_dict) \
                        and (self.feature_statistics.spelling_suffix_count_dict[(cur_word[-i-1:], cur_tag)] >= self.threshold):
                    self.suffix_tag_dict[(cur_word[-i-1:], cur_tag)] = self.n_total_features + self.n_suffix_tag
                    self.n_suffix_tag += 1      
        self.n_total_features = self.n_total_features + self.n_suffix_tag       


    def get_trigram_tags_pairs(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
            splited_words = split(line)
            del splited_words[-1]
            for word_idx in range(len(splited_words)):
                  cur_word, cur_tag = split1(splited_words[word_idx]) 
                  if word_idx>1:
                    ptag =  split1(splited_words[word_idx-1])[1]
                    pptag =  split1(splited_words[word_idx-2])[1]
                  elif word_idx == 1:
                    ptag =  split1(splited_words[word_idx-1])[1]
                    pptag = '*'
                  else:    
                    ptag = '*'
                    pptag = '*'
                  if ((pptag,ptag,cur_tag) not in self.trigram_tags_dict) \
                        and (self.feature_statistics.trigram_tags_count_dict[(pptag,ptag,cur_tag)] >= self.threshold):
                    self.trigram_tags_dict[(pptag,ptag,cur_tag)] = self.n_total_features + self.n_trigram_tags
                    self.n_trigram_tags += 1
        self.n_total_features = self.n_total_features + self.n_trigram_tags



    def get_bigram_tags_pairs(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
            splited_words = split(line)
            del splited_words[-1]
            for word_idx in range(len(splited_words)):
                  cur_word, cur_tag = split1(splited_words[word_idx]) 
                  if word_idx>0:
                    ptag =  split1(splited_words[word_idx-1])[1]
                  else:    
                    ptag = '*'
                  if ((ptag,cur_tag) not in self.bigram_tags_dict) \
                        and (self.feature_statistics.bigram_tags_count_dict[(ptag,cur_tag)] >= self.threshold):
                    self.bigram_tags_dict[(ptag,cur_tag)] = self.n_total_features + self.n_bigram_tags
                    self.n_bigram_tags += 1
        self.n_total_features = self.n_total_features + self.n_bigram_tags


    def get_unigram_tags_pairs(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
            splited_words = split(line)
            del splited_words[-1]
            for word_idx in range(len(splited_words)):
                  cur_word, cur_tag = split1(splited_words[word_idx]) 
                  if ((cur_tag) not in self.unigram_tags_dict) \
                        and (self.feature_statistics.unigram_tags_count_dict[(cur_tag)] >= self.threshold):
                    self.unigram_tags_dict[(cur_tag)] = self.n_total_features + self.n_unigram_tags
                    self.n_unigram_tags += 1
        self.n_total_features = self.n_total_features + self.n_unigram_tags


    # --- ADD YOURE CODE BELOW --- #

class history_feature_class():

    def __init__(self, feature2id, file_path,tags_list):
        #self.word_tags_dict = feature2id.word_tags_dict
        #self.prefix_tag_dict = feature2id.prefix_tag_dict
        self.feature2id = feature2id
        self.word_features_list=[]
        self.word_tags_features_list=[]
        self.tags_list = tags_list


    def get_history(self, file_path):
        """
            Extract out of text all word/tag pairs
            :param file_path: full path of the file to read
                return all word/tag pairs with index of appearance
        """
        with open(file_path) as f:
          for line in f:
            splited_words = split(line)
            del splited_words[-1]
            pptag='*'
            ptag='*'
            pword='*'
            length=len(splited_words)
            for word_idx in range(length):
                if word_idx>1: 
                  ptag= split1(splited_words[word_idx-1])[1]
                  pword=split1(splited_words[word_idx-1])[0]
                  pptag= split1(splited_words[word_idx-2])[1]
                elif word_idx==1:
                  ptag= split1(splited_words[word_idx-1])[1]
                  pword=split1(splited_words[word_idx-1])[0]
                word, ctag = split1(splited_words[word_idx])
                if word_idx == length-1:
                  ntag='*'
                  nword='*'
                else:
                  ntag= split1(splited_words[word_idx+1])[1]
                  nword=split1(splited_words[word_idx+1])[0]
                history=(word,ptag,ntag,ctag,pword,nword,pptag)
                self.word_features_list.append((word,ctag,represent_input_with_features(history,self.feature2id)))
                word_featurs_per_tag = []
                for tag in self.tags_list:
                  history=(word,ptag,ntag,tag,pword,nword,pptag)
                  word_featurs_per_tag.append(represent_input_with_features(history,self.feature2id))
                self.word_tags_features_list.append((word,word_featurs_per_tag))  


    # --- ADD YOURE CODE BELOW --- #

"""### Representing input data with features 
After deciding which features to use, we can represent input tokens as sparse feature vectors. This way, a token is represented with a vec with a dimension D, where D is the total amount of features. \
This is done at training step.

### History tuple
We define a tuple which hold all relevant knowledge about the current word, i.e. all that is relevant to extract features for this token.

$$History = (W_{cur}, T_{prev}, T_{next}, T_{cur}, W_{prev}, W_{next}) $$
"""

def represent_input_with_features(history, feature2id):
    """
        Extract feature vector in per a given history
        :param history: touple{word, pptag, ptag, ctag, nword, pword}
        :param word_tags_dict: word\tag dict
            Return a list with all features that are relevant to the given history
    """
    word = history[0]
    ptag = history[1]
    ntag = history[2]
    ctag = history[3]
    pword = history[4]
    nword = history[5]
    pptag = history[6]
    features = []

#word-tag

    if (word, ctag) in feature2id.words_tags_dict:
        features.append(feature2id.words_tags_dict[(word, ctag)])

#prefix-tag
    for i in range (min(4,len(word))): 
      if (word[:i+1], ctag) in feature2id.prefix_tag_dict:
        features.append(feature2id.prefix_tag_dict[(word[:i+1], ctag)])
        
#suffix-tag
    for i in range (min(4,len(word))): 
      if (word[-i-1:], ctag) in feature2id.suffix_tag_dict:
        features.append(feature2id.suffix_tag_dict[(word[-i-1:], ctag)])

#trigram tags
    if (pptag,ptag,ctag) in feature2id.trigram_tags_dict:
        features.append(feature2id.trigram_tags_dict[(pptag,ptag,ctag)])

#bigram tags
    if (ptag,ctag) in feature2id.bigram_tags_dict:
        features.append(feature2id.bigram_tags_dict[(ptag,ctag)])   

#bigram tags
    if (ctag) in feature2id.unigram_tags_dict:
        features.append(feature2id.unigram_tags_dict[(ctag)])      
    
    # --- CHECK APEARANCE OF MORE FEATURES BELOW --- #
    
    return features

"""## Part 2 - Optimization

Recall from tutorial that the log-linear objective is: \
$$(1)\textbf{   } L(v)=\underbrace{\sum_{i=1}^{n} v\cdot f(x_{i},y_{i})}_\text{Linear Term}-\underbrace{\sum_{i=1}^{n}\log(\sum_{y'\in Y} e^{v\cdot f(x_{i},y')})}_\text{Normalization Term} - \underbrace{0.5\cdot\lambda\cdot\|v \|^{2}}_\text{Regularization}$$ 
Where $v$ represents the model weights, $x_{i}$ is the $i'th$ input token, $y_{i}$ is the $i'th$ label and $Y$ represents all possible labels. \
The corresponding gradient is:
$$(2)\textbf{}\frac{\partial L(v)}{\partial v} = \underbrace{\sum_{i=1}^{n} f(x_{i},y_{i})}_\text{Empirical Counts} - \underbrace{\sum_{i=1}^{n}\sum_{y'\in Y} f(x_{i},y') \cdot p(y' | x_{i} ; v)}_\text{Expected Counts} - \underbrace{\lambda\cdot v}_\text{Reg Grad} $$

### How to speed up optimization 
Gradient descent is an iterative optimization method. The log-linear objective presented in equation (1) is a convex problem, which guaranties a convergence for gradient descent iterations (when choosing an appropriate step size). However, in order to converge, many iterations must be performed. Therefore, it is importent to (1) speed up each iteration and to (2) decrease the number of iterations.


#### Decrease the number of iterations 
Notice that by increasing $\lambda$ we can force the algorithm to search for a solution in a smaller search space - which will reduce the number of iterations. However, this is a tredoff, because it will also damage train-set accuracy (Notice that we don't strive to achieve 100% accuracy on the training set, as sometimes by reducing training accuracy we achieve improvement in developement set accuracy).      


#### Decrease iteration duration
Denote the GD update as:
$$ (3) \textbf{  } v_{k+1} = v_{k} + d \cdot \frac{\partial L}{\partial v} $$
where $v_{k}$ is the weight vector at time $k$, $d$ is a constant step size and $L$ is the objective presentend in equation (1).\
In this excersice we are using `fmin_l_bfgs_b`, which is imported from `scipy.optimize`. This is an iterative optimization function which is similar to GD. The function receives 3 arguments:
 

1.   **func** - a function that clculates the objective and its gradient each iteration.
2.   **x0** - initialize values of the model weights.
3.   **args** - the arguments which 'func' is expecting, except for the first argument - which is the model weights.
4.   **maxiter** (optional) - specify a hard limit for number of iterations. (int)
5.   **iprint**  (optional) - specify the print interval (int) 0 < iprint < 99 



Think of ways to efficiently calculate eqautions (1) and (2) according to your features implementation. Furthermore, think which parts must be computed in each iteration, and whether others can be computed once.
"""

def calc_objective_per_iter(w_i, args1,args2,args3,args4,args5):
    """
        Calculate max entropy likelihood for an iterative optimization method
        :param w_i: weights vector in iteration i 
        :param arg_i: arguments passed to this function, such as lambda hyperparameter for regularization
        
            The function returns the Max Entropy likelihood (objective) and the objective gradient
    """

    ## Calculate the terms required for the likelihood and gradient calculations
    ## Try implementing it as efficient as possible, as this is repeated for each iteration of optimization.
    #linear term
    linear_term = 0
    for i in range (args4):
      for feature in args1[i][2]:
        linear_term+=w_i[feature]
      
    #normalization term
    num_tags=args3
    normalization_term=0  
    for i in range (args4):
      sum_all_tags=0
      for j in range (num_tags):
        sum_tag=0
        for feature in args2[i][1][j]:
          sum_tag+=w_i[feature]
        sum_all_tags+=math.exp(sum_tag) 
      normalization_term+=math.log(sum_all_tags)

    #regularization
    length=len(w_i)
    regularization=0
    for i in range(length):
      regularization+=w_i[i]**2
    regularization=0.5*regularization

    #empirical counts
    empirical_counts=np.zeros(args5, dtype=np.float32)
    for i in range (args4):
      for feature in args1[i][2]:
        empirical_counts[feature]+=1

    #expected counts
    expected_counts=np.zeros(args5, dtype=np.float32)
    for i in range (args4):
      denominator=0
      for k in range(num_tags):
        sum_tag=0
        for feature in args2[i][1][k]:
          sum_tag+=w_i[feature]
          
        denominator+=math.exp(sum_tag)  
      for j in range(num_tags):
        sum_tag=0
        for feature in args2[i][1][j]:
          sum_tag+=w_i[feature]
        numerator=math.exp(sum_tag)
        for feature in args2[i][1][j]:
          expected_counts[feature]+=numerator/denominator

    #regularization grad
    regularization_grad=w_i




    likelihood = linear_term - normalization_term - regularization
    grad = empirical_counts - expected_counts - regularization_grad
    print("like = ",likelihood)
    return (-1)*likelihood, (-1)*grad

"""Now lets run the code untill we get the optimized weights."""

from scipy.optimize import fmin_l_bfgs_b

# Statistics
statistics = feature_statistics_class()
statistics.get_word_tag_pair_count('/content/train1.wtag')
statistics.get_spelling_prefix_count('/content/train1.wtag')
statistics.get_spelling_suffix_count('/content/train1.wtag')
statistics.get_trigram_tags_count('/content/train1.wtag')
statistics.get_bigram_tags_count('/content/train1.wtag')
statistics.get_unigram_tags_count('/content/train1.wtag')




# feature2id
threshold=3
feature2id = feature2id_class(statistics, threshold)
feature2id.get_word_tag_pairs('/content/train1.wtag')
feature2id.get_prefix_tag_pairs('/content/train1.wtag')
feature2id.get_suffix_tag_pairs('/content/train1.wtag')
feature2id.get_trigram_tags_pairs('/content/train1.wtag')
feature2id.get_bigram_tags_pairs('/content/train1.wtag')
feature2id.get_unigram_tags_pairs('/content/train1.wtag')



tags_list=list(get_tags_list('/content/train1.wtag'))
history_feature=history_feature_class(feature2id,'/content/train1.wtag',tags_list)
history_feature.get_history('/content/train1.wtag')
args1=history_feature.word_features_list
args2=history_feature.word_tags_features_list
args4=len(args1)
args5=feature2id.n_total_features
args3=len(tags_list)
args=(args1,args2,args3,args4,args5)
# define 'args', that holds the arguments arg_1, arg_2, ... for 'calc_objective_per_iter'
#args = (arg_1, arg_2, ...)
w_0 = np.zeros(feature2id.n_total_features, dtype=np.float32)
optimal_params = fmin_l_bfgs_b(func=calc_objective_per_iter, x0=w_0, args=args, maxiter=1000, iprint=1)
weights = optimal_params[0]

# Now you can save weights using pickle.dump() - 'weights_path' specifies where the weight file will be saved.
# IMPORTANT - we expect to recieve weights in 'pickle' format, don't use any other format!!
weights_path = 'your_path_to_weights_dir/trained_weights_data_i.pkl' # i identifies which dataset this is trained on
with open(weights_path, 'wb') as f:
    pickle.dump(optimal_params, f)

#### In order to load pre-trained weights, just use the next code: ####
#                                                                     #
# with open(weights_path, 'rb') as f:                                 #
#   optimal_params = pickle.load(f)                                   #
# pre_trained_weights = optimal_params[0]                             #
#                                                                     #
#######################################################################

"""## Part 3 - Inference with MEMM-Viterbi
Recall - the MEMM-Viterbi takes the form of:
\begin{align*}
\textbf{Base case: }~~~~~~ &                       \\
                           & \pi(0, *, *)=1        \\
                                                      &                       \\
\textbf{The recursive}     & \textbf{ definition:} \\
                           & \text{For any } k\in\{1,..., n\} ,\text{for any } u \in S_{k-1} \text{ and } v \in S_{k}:\\
                           & ~~~~~~~~~~~~~~~\pi(k, u, v)=\max_{t\in S_{k-2}}{\{\pi(k-1, t, u)\cdot q(v|t, u, w_{[1:n]}, k)\}}\\
\end{align*}
where $S_{k}$ is a set of possible tags at position $k$.
"""

def memm_viterbi():
    """
    Write your MEMM Vitebi imlementation below
    You can implement Beam Search to improve runtime
    Implement q efficiently (refer to conditional probability definition in MEMM slides)
    """

    return tags_infer

from IPython.display import HTML
from base64 import b64encode
! git clone https://github.com/eyalbd2/097215_Natural-Language-Processing_Workshop-Notebooks.git

mp4 = open('/content/097215_Natural-Language-Processing_Workshop-Notebooks/ViterbiSimulation.mp4','rb').read()
data_url = "data:video/mp4;base64," + b64encode(mp4).decode()

HTML("""
<video width=1000 controls>
      <source src="%s" type="video/mp4">
</video>
""" % data_url)

"""Notation:
  - *w* refers to the trained weights vector
  - *u* refers to the previous tag
  - *v* refers to the current tag
  - *t* refers to the tag previous to *u*

The video above presents a vanilla memm viterbi. \
There are several methods to improve the performence of the algorithm, we will specify two of them: 


1.   Dividing the algorithm to multiple processes 
2.   Implementing beam search viterbi, and reducing Beam size 


Notice that the latter might affect the results, hence beam size is required to be chosen wisely.

## Accuracy and Confusion Matrix
![Accuracy and Confusion Matrix](https://raw.githubusercontent.com/eyalbd2/097215_Natural-Language-Processing_Workshop-Notebooks/master/conf_mat_slide.PNG)

## Interface for creating competition files
In your submission, you must implement a python file named `generate_comp_tagged.py` which generates your tagged competition files for both datasets in a single call.
It should do the following for each dataset:

1. Load trained weights to your model
2. Load competition data file
3. Run inference on competition data file
4. Write results to file according to .wtag format (described in HW1)
5. Validate your results file comply with .wtag format (according to instructions described in HW1)

## <img src="https://img.icons8.com/dusk/64/000000/prize.png" style="height:50px;display:inline"> Credits
* Special thanks to <a href="mailto:taldanielm@campus.technion.ac.il">Tal Daniel</a> , specifically for his viterbi implementation
* By <a href="https://github.com/eyalbd2">Eyal Ben David</a> and <a href="https://github.com/nadavo">Nadav Oved </a>
"""